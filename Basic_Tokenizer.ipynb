{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-3LVaHF-fnf",
        "outputId": "dba2ad87-0dc3-4ec5-db0e-dec242d79305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic Tokenizer\n",
            "1. Enter text manually\n",
            "2. Load text from a file\n",
            "Choose an option (1 or 2): 2\n",
            "Enter the full path of the text file: /content/Friendship.txt\n",
            "\n",
            "Tokens: ['friendship', 'â€“', \"doraemon's\", 'worlddoraemon', 'and', 'nobita', 'share', 'a', 'heartwarming', 'and', 'iconic', 'friendship', 'built', 'on', 'mutual', 'support', 'and', 'understanding', 'doraemon', 'a', 'robotic', 'cat', 'from', 'the', 'future', 'is', 'sent', 'to', 'help', 'nobita', 'a', 'clumsy', 'and', 'often', 'unlucky', 'boy', 'with', 'his', 'challenges', 'while', \"doraemon's\", 'gadgets', 'often', 'play', 'a', 'comedic', 'role', 'they', 'also', 'symbolize', 'his', 'dedication', 'to', 'making', \"nobita's\", 'life', 'better', 'and', 'helping', 'him', 'learn', 'valuable', 'lessons', 'their', 'bond', 'highlights', 'themes', 'of', 'friendship', 'loyalty', 'and', 'the', 'importance', 'of', 'believing', 'in', 'oneself']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def simple_tokenizer(text):\n",
        "    \"\"\"\n",
        "    Tokenizes input text by removing basic punctuation and splitting by spaces.\n",
        "    \"\"\"\n",
        "    punctuation = \"!.,?;:\\\"()[]{}<>\"\n",
        "    for char in punctuation:\n",
        "        text = text.replace(char, \"\")\n",
        "    text = text.lower()\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to control user input and display the tokenized output.\n",
        "    \"\"\"\n",
        "    print(\"Basic Tokenizer\")\n",
        "    print(\"1. Enter text manually\")\n",
        "    print(\"2. Load text from a file\")\n",
        "\n",
        "    choice = input(\"Choose an option (1 or 2): \")\n",
        "\n",
        "    if choice == \"1\":\n",
        "        user_text = input(\"Enter the text to tokenize: \")\n",
        "    elif choice == \"2\":\n",
        "        file_path = input(\"Enter the full path of the text file: \")\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                user_text = file.read()\n",
        "        except FileNotFoundError:\n",
        "            print(\"File not found. Please check the path and try again.\")\n",
        "            return\n",
        "    else:\n",
        "        print(\"Invalid option. Please enter 1 or 2.\")\n",
        "        return\n",
        "\n",
        "    tokens = simple_tokenizer(user_text)\n",
        "    print(\"\\nTokens:\", tokens)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "# Additional Functions (Optional)\n",
        "def show_instructions():\n",
        "    print(\"\\n--- Instructions ---\")\n",
        "    print(\"This program tokenizes your input text.\")\n",
        "    print(\"You can either type your own text or provide a file path.\\n\")\n"
      ]
    }
  ]
}